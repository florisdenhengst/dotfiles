import cvxopt as co
from kernel import Kernel
import numpy as np
import matplotlib.pyplot as plt
import math
import pprint
import sklearn as sk
from sklearn import grid_search
from svdd import SVDD
import sklearn
from sklearn.utils.estimator_checks import check_estimator
from sklearn.utils import check_array
from sklearn.utils.validation import column_or_1d
import sys
from ssad_wrapper import *
from midpoint_normalize import *
import json
import os
import time
from optimals import optimals

POS_LABEL = -1
K_TYPE = 'rbf'
USAGE = '''Usage: python learning_curve_intra.py 
  <method>
  <data_file>
  <unsup_scores_file>
  <classes_file>
  <json_output_prefix>
  <?imgs_to_dir>
  <?n_jobs>
  <?n_folds>
  '''
co.solvers.options['show_progress'] = False
co.cvxprog.options['show_progress'] = False
co.coneprog.options['show_progress'] = False

def rank_based_disagreement(ranks1, ranks2, ignore = None):
  '''Feed in JUST THE RANKS (i.e. not the indices).
  Returns index of element which has the highest rank-based difference.'''
  assert ranks1.shape == ranks2.shape
  rank_diffs = np.abs(ranks1 - ranks2)
  if ignore is not None:
    # disable everything in selection
    rank_diffs[ignore] = float('-inf')
  return np.argmax(rank_diffs)

def ranked_indices(scores_to_rank):
  # append indices column-wise
  scores_to_rank = np.transpose(np.vstack((scores_to_rank, np.arange(0,scores_to_rank.shape[0]))))
  # now sort them
  scores_to_rank = scores_to_rank[scores_to_rank[:,0].argsort(axis=0)]
  # append new indices to denote ranks
  scores_to_rank = np.transpose(np.vstack((scores_to_rank[:,0],
                                         scores_to_rank[:,1],
                                         np.arange(0,scores_to_rank.shape[0]))))
  # reorder to get original ordering
  ranks = scores_to_rank[scores_to_rank[:,1].argsort(axis=0)][:,2:3]
  return ranks

def assign_optional_arg(index, argname):
  try:
    globals()[argname] = sys.argv[index]
  except IndexError:
    print('No arg {} provided.'.format(argname))

if len(sys.argv) < 5:
  print()

try:
  method = sys.argv[1]
  data_file = sys.argv[2]
  unsup_scores_file = sys.argv[3]
  classes_file = sys.argv[4]
  output_file = sys.argv[5]
except IndexError:
  print(USAGE)
  exit(1)

for i, arg in enumerate(['img_to_dir', 'n_jobs', 'n_folds']):
  assign_optional_arg(i+6, arg)


# read input file 
data = np.loadtxt(open(data_file,'rb'), delimiter=',')

settings = None
datasets = ['usg', 'abalone', 'thyroid']
for dataset in datasets:
  if dataset in data_file:
    print("Detected '{}' as dataset.".format(dataset))
    settings = optimals[dataset]
    break

# read classes file
trues = np.loadtxt(open(classes_file, 'rb'), delimiter=',')[:,1]
y_true = trues
assert np.min(y_true) == -1
assert np.max(y_true) == 1
assert len(np.unique(y_true)) == 2


# read unsupervised file
unsup_scores = np.loadtxt(open(unsup_scores_file, 'rb'), delimiter=',')[:,1]
assert np.min(unsup_scores) > 0

# validate json output
with open(output_file, 'w+') as f:
  print('output file validated')

unsup_ranks = ranked_indices(unsup_scores)

# validate scores and classes etc.
assert len(trues) == len(data[:,0]) == len(unsup_scores)
assert len(np.unique(trues)) == 2

# set optimal parameters
if settings is None:
  settings = {
      'ssad': {
        'C_base': 0.0005,
        'k_param': 0.0003,
        },
      'svc': {
        'C': 35.6225,
        'gamma': 0.1374,
        'probability': True,
        }
      }
  print('Using default settings')
if method == 'svc':
  model_t = sk.svm.SVC
elif method == 'ssad':
  model_t = SSAD_WRAPPER
else:
  print("Unknown method '{}'!".format(method))
  exit(1)

# TODO
hints_range = range(len(trues))
print('hints range len', len(hints_range))
scorer = sk.metrics.recall_score

y_working = np.zeros(len(trues))
working_unsup_scores = unsup_scores
assert y_working.shape == trues.shape

ns_hints = []
scores = []
for i, n_hints in enumerate(hints_range):
  assert y_working.shape == trues.shape
  # build model
  model = model_t(**settings[method])

  # remove all entries for which no label is present and store in separate object for SVC
  labelled = y_working != 0
  unlabelled = np.logical_not(labelled)

  assert sum(y_working == -1) + sum(y_working == 1) == n_hints
  assert sum(y_working == 0) + n_hints == len(trues)

  unsup_query_index = np.argmax(working_unsup_scores)

  # set up training data
  if method == 'ssad':
    # ssad can use all instances, but not all y_true
    training_set = data
    # deep copy to solve heisenbug?
    y_training = np.empty_like(y_working)
    y_training[:] = y_working
    test_set = data
    y_test = y_true
  else:
    # svc can only use labelled instances
    training_set = data[labelled,:]
    y_training = y_working[labelled]
    test_set = data
    y_test = y_true

  if method == 'ssad' or (method == 'svc' and len(np.unique(y_training)) == 2):
    if method == 'svc' or len(np.unique(y_training)) > 2:
      # svm requires both negative and positive labels
      assert y_training.max() == 1
      assert y_training.min() == -1
    model = model.fit(training_set, y_training)
  elif np.unique(y_training) > 2:
    print('More than 2 classes known!!')
    exit(1)
  else:
    # we cannot fit the model, leave it as-is
    pass

  sup_min_distance_index = None
  sup_query_index = None

  # query in alternating fashion.
  if i % 2 == 0:
    # 'margin' strategy <- These are points about which 'supervised component' is uncertain
    # TODO: process decision_function output correctly
    try:
      sklearn.utils.validation.check_is_fitted(model, 'support_')
      sup_distances = model.decision_function(test_set)
      assert len(sup_distances) == len(test_set)
      assert len(sup_distances) == len(y_true)
      # clamp distances for labelled values to 'inf' in order to be able to select the minimum
      assert sup_distances.max() < float('inf')
      sup_distances[labelled] = float('inf')
      sup_min_distance_index = np.argmin(sup_distances)
    except sklearn.utils.validation.NotFittedError:
      # default to the most suspected outlier in order to facilitate quick inclusion of the
      # fully-supervised method
      print('Model not fitted yet!')
      sup_min_distance_index = None

    query_index = sup_min_distance_index or unsup_query_index
  else:
    # 'rank based disagreement' strategy
    try:
      sklearn.utils.validation.check_is_fitted(model, 'support_')
      sup_distances = model.decision_function(test_set)
      sup_ranked = ranked_indices(sup_distances)
      sup_query_index = rank_based_disagreement(unsup_ranks, sup_ranked, labelled)
    except sklearn.utils.validation.NotFittedError:
      # default to the most suspected outlier in order to facilitate quick inclusion of the
      # fully-supervised method
      print('Model not fitted yet!')
      sup_query_index = None

    query_index = sup_query_index or unsup_query_index

  # add label of hinting point to known labels
  # set distance of working_unsup_scores to -1
  assert query_index <= len(data[:,0])
  assert working_unsup_scores[query_index] != -1
  assert y_working[query_index] == 0
  y_working[query_index] = y_true[query_index]
  assert y_working[query_index] != 0
  working_unsup_scores[query_index] = -1

  # measure performances (F1 score)
  try:
    sklearn.utils.validation.check_is_fitted(model, 'support_')
    # predict on all points
    predictions = model.predict(data)
    score = scorer(y_true, predictions, pos_label=POS_LABEL)
  except sklearn.utils.validation.NotFittedError:
    score = 0

  print(n_hints, score)
  # store result
  scores.append(score)
  ns_hints.append(n_hints)

percentage_labelled = np.divide(ns_hints, data.shape[0])
if img_to_dir is not None:
  img_out = img_to_dir + '/inner_output_' + method + '_recall.pdf'

plt.figure()
plt.title('learning curve')
plt.xlabel('% queried')
plt.ylabel('Recall')
plt.plot(percentage_labelled, scores)
plt.ylim(0, 1)
if img_to_dir is not None:
  plt.savefig(img_out, format='pdf')
else:
  plt.show(img_to)

# output json for ssad to file
dump = json.dumps({'ns_hints': ns_hints, 'scores': scores, 'scorer': scorer.__name__},
                  indent=2,
                  sort_keys=True)

# output json for svc to file
with open(output_file, 'w+') as f:
  print(dump, file=f)

# create learning plots



# output learning plot for ssad to file / show

# output learning plot for svc to file / show
