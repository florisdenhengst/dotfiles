import cvxopt as co
from kernel import Kernel
import numpy as np
import matplotlib.pyplot as plt
import pprint
import sklearn as sk
from sklearn import grid_search
from ssad import SSAD
from sklearn.utils.estimator_checks import check_estimator
from sklearn.utils import check_array
from sklearn.utils.validation import column_or_1d
import sys
import six
from collections import Counter

class SSAD_WRAPPER(sk.base.BaseEstimator,
                   sk.base.ClassifierMixin):
  C_base = None
  model = None
  k_type = None
  k_param = None
  fitting_data = None
  kappa = None
  _Cp = None
  _Cu = None
  _Cn = None
  balanced = None
  classes_pct = {1: 0.98, -1:0.02}

  def __init__(self, k_type='rbf', k_param=1.0, C_base=1.0, kappa=1.0, classes_pct={1: 0.98, -1:
      0.02}, balanced=False):
    self.k_type = k_type
    self.kappa = kappa
    self.C_base = C_base
    self.k_param = k_param
    self.classes_pct = classes_pct
    self.balanced = balanced

  def set_params(self, **params):
    """Set the parameters of this estimator.

    The method works on simple estimators as well as on nested objects
    (such as pipelines). The former have parameters of the form
    ``<component>__<parameter>`` so that it's possible to update each
    component of a nested object.

    Returns
    -------
    self
    """
    if not params:
      # Simple optimisation to gain speed (inspect is slow)
      return self
    valid_params = self.get_params(deep=True)
    for key, value in six.iteritems(params):
      split = key.split('__', 1)
      if len(split) > 1:
        # nested objects case
        name, sub_name = split
        if name not in valid_params:
          raise ValueError('Invalid parameter %s for estimator %s. '
                           'Check the list of available parameters '
                           'with `estimator.get_params().keys()`.' %
                           (name, self))
        sub_object = valid_params[name]
        sub_object.set_params(**{sub_name: value})
      else:
        # simple objects case
        if key not in valid_params:
          raise ValueError('Invalid parameter %s for estimator %s. '
                           'Check the list of available parameters '
                           ': 0.98, -1:0.02with `estimator.get_params().keys()`.' %
                           (key, self.__class__.__name__))
        setattr(self, key, value)
    return self

  def set_classes_pcts(self, y):
    ds_size = y.shape[0]
    y = y.tolist()
    counter = Counter(y)
    self.classes_pct[-1] = counter[-1] / ds_size
    self.classes_pct[1] = counter[1] / ds_size

    summed = 0
    assert sum([val for key, val in self.classes_pct.items()]) == 1.0

  def set_Cs(self, y, X):
    # update Cp, Cu, Cn
    if self.balanced:
      self.set_classes_pcts(y)

    if y is None:
      self._Cp = self._Cu = self._Cn = self.C
    else:
      # some ys are give. Update the C's to reflect their probabilities:
      n_unknown = (y == 0).sum()
      n_pos = (y == -1).sum()
      n_neg = (y == 1).sum()
      assert n_unknown + n_pos + n_neg == y.size
    self._Cu = self.C_base / (self.C_base / 100) * self.C_base
    if self.balanced:
      ds_size = X.shape[0]
      expected_n_pos = self.classes_pct[1] * ds_size
      self._Cp = self.C_base * (expected_n_pos / ds_size)
      expected_n_neg = self.classes_pct[-1] * ds_size
      self._Cn = self.C_base * (expected_n_neg / ds_size)
    else:
      self._Cp = self.C_base
      self._Cn = self.C_base

  def fit(self, X, y):
    X = check_array(X)
    if y is not None:
      y = column_or_1d(y)
      assert y.shape[0] == X.shape[0]
    if y is not None and any([X.shape[0] == 0,
            X.shape[1] == 0,
            len(y) != X.shape[0]]):
      msg = '0 feature(s) (shape=(3, 0)) while a minimum of 1 is required.'
      raise ValueError(msg)
    self.set_Cs(y, X)
    _y = co.matrix(np.array(y).astype(float)).trans()
    _X = co.matrix(np.transpose(X.astype(float)))
    self.training_dim = len(_X[:,0])
    kernel = Kernel.get_kernel(_X, _X, type=self.k_type, param=self.k_param)
    assert kernel.size[0] > 0
    self.model = SSAD(kernel, _y, self.kappa, self._Cp, self._Cu, self._Cn)
    result = self.model.train_dual()
    assert result == SSAD.MSG_OK
    self.threshold = self.model.get_threshold()[0]
    assert self.threshold is not None
    assert self.model.get_support_dual() is not None
    self.fitting_data = _X
    self.support_ = True
    return self

  def get_threshold(self):
    """Returns optimal decision threshold for a trained model."""
    assert self.fitting_data is not None
    return self.model.get_threshold()[0]

  def decision_function(self, X):
    """Gives distances from decision boundary."""
    pred = self.predict_proba(X)[0]
    thresholds = np.array([self.get_threshold()] * pred.shape[0])
    return np.abs(pred - thresholds)

  def predict_proba(self, X):
    """gives probabilities for positive (i.e. anomalous class)."""
    X = check_array(X)
    _X = co.matrix(np.transpose(X.astype(float)))
    assert self.fitting_data is not None
    Y = self.fitting_data[:, self.get_support_dual()]
    kernel = Kernel.get_kernel(
        _X,
        Y,
        self.k_type,
        self.k_param)
    assert self.training_dim == len(Y[:,0])
    assert self.training_dim == len(_X[:,0])
    norms = Kernel.get_diag_kernel(_X, self.k_type, self.k_param)
    (pred, msg) = self.model.apply_dual(kernel)
    assert msg == SSAD.MSG_OK
    assert len(pred) == len(X[:,0])
    pred = np.array(pred)
    return pred

  def predict(self, X):
    probs = self.predict_proba(X)[:,0]
    booleans = [p > self.threshold - SSAD.PRECISION for p in probs]
    return [-1 if not b else 1 for b in booleans]

  def get_support_dual(self):
    return self.model.get_support_dual()

