import cvxopt as co
from kernel import Kernel
import numpy as np
import matplotlib.pyplot as plt
import math
import pprint
import sklearn as sk
from sklearn import grid_search
from svdd import SVDD
import sklearn
from sklearn.utils.estimator_checks import check_estimator
from sklearn.utils import check_array
from sklearn.utils.validation import column_or_1d
import sys
from ssad_wrapper import *
from midpoint_normalize import *
import json
import os
import time
from optimals import optimals
import random

POS_LABEL = -1
K_TYPE = 'rbf'
N_ITERATIONS = 2
USAGE = '''Usage: python learning_curve_intra_random.py 
  <method>
  <data_file>
  <classes_file>
  <json_output_prefix>
  <?imgs_to_dir>
  <?n_jobs>
  <?n_folds>
  '''
co.solvers.options['show_progress'] = False
co.cvxprog.options['show_progress'] = False
co.coneprog.options['show_progress'] = False

def rank_based_disagreement(ranks1, ranks2, ignore = None):
  '''Feed in JUST THE RANKS (i.e. not the indices).
  Returns index of element which has the highest rank-based difference.'''
  assert ranks1.shape == ranks2.shape
  rank_diffs = np.abs(ranks1 - ranks2)
  if ignore is not None:
    # disable everything in selection
    rank_diffs[ignore] = float('-inf')
  return np.argmax(rank_diffs)

def ranked_indices(scores_to_rank):
  # append indices column-wise
  scores_to_rank = np.transpose(np.vstack((scores_to_rank, np.arange(0,scores_to_rank.shape[0]))))
  # now sort them
  scores_to_rank = scores_to_rank[scores_to_rank[:,0].argsort(axis=0)]
  # append new indices to denote ranks
  scores_to_rank = np.transpose(np.vstack((scores_to_rank[:,0],
                                         scores_to_rank[:,1],
                                         np.arange(0,scores_to_rank.shape[0]))))
  # reorder to get original ordering
  ranks = scores_to_rank[scores_to_rank[:,1].argsort(axis=0)][:,2:3]
  return ranks

def assign_optional_arg(index, argname):
  try:
    globals()[argname] = sys.argv[index]
  except IndexError:
    print('No arg {} provided.'.format(argname))

if len(sys.argv) < 5:
  print()

try:
  method = sys.argv[1]
  data_file = sys.argv[2]
  classes_file = sys.argv[3]
  output_file = sys.argv[4]
except IndexError:
  print(USAGE)
  exit(1)

for i, arg in enumerate(['img_to_dir', 'n_jobs', 'n_folds']):
  assign_optional_arg(i+5, arg)


# read input file 
data = np.loadtxt(open(data_file,'rb'), delimiter=',')

settings = None
datasets = ['usg', 'abalone', 'thyroid']
for dataset in datasets:
  if dataset in data_file:
    print("Detected '{}' as dataset.".format(dataset))
    settings = optimals[dataset]
    break

# read classes file
trues = np.loadtxt(open(classes_file, 'rb'), delimiter=',')[:,1]
y_true = trues
assert np.min(y_true) == -1
assert np.max(y_true) == 1
assert len(np.unique(y_true)) == 2


# validate json output
with open(output_file, 'w+') as f:
  print('output file validated')

# validate scores and classes etc.
assert len(np.unique(trues)) == 2

# set optimal parameters
if settings is None:
  settings = {
      'ssad': {
        'C_base': 0.0005,
        'k_param': 0.0003,
        },
      'svc': {
        'C': 35.6225,
        'gamma': 0.1374,
        'probability': True,
        }
      }
  print('Using default settings')
if method == 'svc':
  model_t = sk.svm.SVC
elif method == 'ssad':
  model_t = SSAD_WRAPPER
else:
  print("Unknown method '{}'!".format(method))
  exit(1)

# TODO
hints_range = range(len(trues))
print('hints range len', len(hints_range))
scorer = sk.metrics.f1_score

ns_hints = []
scores = []
query_index = None
for iteration in range(N_ITERATIONS):
  scores.append([])
  ns_hints.append([])

  y_working = np.zeros(len(trues))
  assert y_working.shape == trues.shape

  for i, n_hints in enumerate(hints_range):
    assert y_working.shape == trues.shape
    # build model
    model = model_t(**settings[method])

    # remove all entries for which no label is present and store in separate object for SVC
    labelled = y_working != 0
    unlabelled = np.logical_not(labelled)

    assert sum(y_working == -1) + sum(y_working == 1) == n_hints
    assert sum(y_working == 0) + n_hints == len(trues)

    # set up training data
    if method == 'ssad':
      # ssad can use all instances, but not all y_true
      training_set = data
      # deep copy to solve heisenbug?
      y_training = np.empty_like(y_working)
      y_training[:] = y_working
      test_set = data
      y_test = y_true
    else:
      # svc can only use labelled instances
      training_set = data[labelled,:]
      y_training = y_working[labelled]
      test_set = data
      y_test = y_true

    if method == 'ssad' or (method == 'svc' and len(np.unique(y_training)) == 2):
      if method == 'svc' or len(np.unique(y_training)) > 2:
        # svm requires both negative and positive labels
        assert y_training.max() == 1
        assert y_training.min() == -1
      model = model.fit(training_set, y_training)
    elif np.unique(y_training) > 2:
      print('More than 2 classes known!!')
      exit(1)
    else:
      # we cannot fit the model, leave it as-is
      pass

    # query a random unlabelled point.
    possible_query_indices = np.where( y_working == 0 )[0]
    assert possible_query_indices.shape[0] > 0
    random_index = random.randint(0, len(possible_query_indices) - 1)
    query_index = possible_query_indices[random_index]

    # add label of hinting point to known labels
    # set distance of working_unsup_scores to -1
    assert query_index <= len(data[:,0])
    assert y_working[query_index] == 0
    y_working[query_index] = y_true[query_index]
    assert y_working[query_index] != 0

    # measure performances (F1 score)
    try:
      sklearn.utils.validation.check_is_fitted(model, 'support_')
      # predict on all points
      predictions = model.predict(data)
      score = scorer(y_true, predictions, pos_label=POS_LABEL)
    except sklearn.utils.validation.NotFittedError:
      score = 0

    print(n_hints, query_index, score)
    # store result
    scores[iteration].append(score)
    ns_hints[iteration].append(n_hints)

ns_hints = ns_hints[0]

dump = json.dumps({'ns_hints': ns_hints, 'scores': scores, 'scorer': scorer.__name__},
                  indent=2,
                  sort_keys=True)

averaged_scores_iterations = []
for i in range(len(ns_hints)):
  scores_iterations = [score[i] for score in scores]
  print(i)
  print(scores_iterations)
  print(len(scores_iterations))
  averaged_scores_iterations.append(sum(scores_iterations) / len(scores_iterations))

percentage_labelled = np.divide(ns_hints, len(ns_hints))

plt.figure()
plt.title('learning curve')
plt.xlabel('% queried')
plt.ylabel('F1 score')
plt.plot(percentage_labelled, averaged_scores_iterations)
plt.ylim(0, 1)
img_out = img_to_dir + '/intra_output_random_query_' + '_'.join([str(N_ITERATIONS), method]) + '.pdf'
if img_to_dir is not None:
  plt.savefig(img_out, format='pdf')
else:
  plt.show(img_to)

# output json for ssad to file
dump = json.dumps({'ns_hints': ns_hints, 'scores': scores, 'scorer': scorer.__name__},
                  indent=2,
                  sort_keys=True)

# output json for svc to file
with open(output_file, 'w+') as f:
  print(dump, file=f)

# create learning plots



# output learning plot for ssad to file / show

# output learning plot for svc to file / show
